{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单值 RDD 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map\n",
    "\n",
    "`map` 的用途是将 `RDD` 中的每个元素进行操作(加工), 操作后每个元素得到新的值, 使用这些新的元素生成一个新的 `RDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7] # 1\n",
    "rdd = sc.parallelize(data, 3) # 2\n",
    "arr = rdd.map(lambda x: x + 1).collect() #3\n",
    "print(arr) # 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **创建 python 数组**: 命名为 data\n",
    "2. **创建 RDD 对象**: 使用 SparkContent 的 parallelize 方法将 data 数组分为 3 个 partitions 并返回 RDD 对象赋值给 rdd 变量. *注意: 这里用 sc 表示 SparkContent, 这是因为启动 pyspark 时, 系统自动导入了 SparkContent 类. 其实等于默认执行了 `from pyspark import SparkContent as sc`*\n",
    "3. **创建 python 数组**: 命名为 arr. 指定了一个 lambda 函数(也称之为**匿名函数**), 该函数的作用是对 rdd 中的每个元素进行加 1 操作. 最后的 collect 方法是 RDD 类的方法, 用于将一个 RDD 对象转换为 python 的 数组.\n",
    "4. **控制台输出/打印** python 数组 arr\n",
    "\n",
    "## 综合练习\n",
    "\n",
    "已知文件在hadoop文件系统的 `data/walmart.txt` 中包含 4 列信息, 是沃尔玛全国分店每天的流水, 分别表示分店ID(id), 交易量(x), 交易额(y)和利润(z), 类型均为整数, 数据举例如下\n",
    "\n",
    "|ID|交易量(x)|交易额(y)|利润(z)|\n",
    "|:--|:--|:--|:--|\n",
    "|1|23|5600|5|\n",
    "|2|30|5800|7|\n",
    "|1|27|5000|10|\n",
    "|3|24|6900|5|\n",
    "|2|45|5800|7|\n",
    "|2|28|5800|7|\n",
    "\n",
    "> 请问: 如何得到以下 SQL 等价的结算结果(假设存在表 T 中):\n",
    "\n",
    "```sql\n",
    "select id, sum(x), max(y), average(x) from T group by id;\n",
    "```\n",
    "\n",
    "> 扩展: 如何得到以下 SQL 等价的计算结果(提示, 用 reduce 算子)\n",
    "\n",
    "```sql\n",
    "select sum(x), max(y), min(z), average(x) from T;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '23', '5600', '5'],\n",
       " ['2', '30', '5800', '7'],\n",
       " ['1', '27', '5000', '10'],\n",
       " ['3', '24', '6900', '5'],\n",
       " ['2', '45', '5800', '7'],\n",
       " ['2', '28', '5800', '7']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from file to create RDD, RDD's partitions number is 2\n",
    "walmartRDD = sc.textFile('hdfs://hadoop:9000/data/walmart.txt', 2)\n",
    "\n",
    "# split to list\n",
    "walmartRDD.map(lambda line: line.split(',')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 23), ('2', 30), ('1', 27), ('3', 24), ('2', 45), ('2', 28)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tuple(id, x) x=交易量\n",
    "xRDD = walmartRDD.map(lambda line: line.split(',')) \\\n",
    "    .map(lambda row: (row[0], int(row[1])))\n",
    "xRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 5600), ('2', 5800), ('1', 5000), ('3', 6900), ('2', 5800), ('2', 5800)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tuple(id, y) y=交易额\n",
    "yRDD = walmartRDD.map(lambda line: line.split(',')) \\\n",
    "    .map(lambda row: (row[0], int(row[2])))\n",
    "yRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 5600), ('2', 5800), ('1', 5000), ('3', 6900), ('2', 5800), ('2', 5800)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tuple(id, z) z=利润\n",
    "zRDD = walmartRDD.map(lambda line: line.split(',')) \\\n",
    "    .map(lambda row: (row[0], int(row[2])))\n",
    "zRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 50), ('2', 103), ('3', 24)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum(x) group by id\n",
    "xRDD.reduceByKey(lambda x, y: x + y) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 5600), ('2', 5800), ('3', 6900)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max(y) group by id\n",
    "# way 1\n",
    "yRDD.groupByKey() \\\n",
    "    .mapValues(max) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 5600), ('2', 5800), ('3', 6900)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max(y) group by id\n",
    "# way 2\n",
    "seqOp = (lambda x, y: y if x < y else x)\n",
    "combOp = (lambda x, y: y if x < y else x)\n",
    "yRDD.aggregateByKey(0, seqOp, combOp) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **上面有两种方法求最大值, 第二种方法在性能上要远远优于第一种**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 5000), ('2', 5800), ('3', 6900)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min(z) group by id\n",
    "# way 1\n",
    "zRDD.groupByKey() \\\n",
    "    .mapValues(min) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 5000), ('2', 5800), ('3', 6900)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min(z) group by id\n",
    "# way 2\n",
    "import sys\n",
    "seqOp = (lambda x, y: y if x > y else x)\n",
    "combOp = (lambda x, y: y if x > y else x)\n",
    "zRDD.aggregateByKey(sys.maxsize, seqOp, combOp) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **上面有两种方法求最大值, 第二种方法在性能上要远远优于第一种**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 25.0), ('2', 34.333333333333336), ('3', 24.0)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average(x) group by id\n",
    "seqOp = (lambda acc, val: (acc[0] + val, acc[1] + 1))\n",
    "combOp = (lambda p1, p2: (p1[0] + p2[0], p1[1] + p2[1]))\n",
    "xRDD.aggregateByKey((0, 0), seqOp, combOp) \\             # retrun RDD's element's type is (id, (sum(x), count))\n",
    "    .map(lambda el: (el[0], el[1][0] / el[1][1])) \\      # retrun RDD's element's type is (id, average)\n",
    "    .collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
